% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Machine Learning},
  pdfauthor={Bhaswar Chakma},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Machine Learning}
\author{Bhaswar Chakma}
\date{2021-07-07}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{section}{%
\chapter*{}\label{section}}
\addcontentsline{toc}{chapter}{}

\href{https://cloud.google.com/products/ai/ml-comic-1/}{Source: Google ML Comic}

\hypertarget{ols}{%
\chapter{OLS}\label{ols}}

\hypertarget{slr}{%
\section{SLR}\label{slr}}

We define our column vector of parameters \(\mathbf{w}\)
(\(\boldsymbol{\beta}\) in econometrics):

\[\mathbf{w} = \begin{bmatrix}
           w_{0} \\
           w_{1} 
         \end{bmatrix}\]

A single observation \(\mathbf{x_n}\) is: \[
  \mathbf{x_n} =
  \begin{bmatrix}
          1 \\
          x_{n} 
  \end{bmatrix}
\] And thus the value predicted by the model can be written as:

\[
  f(x_n; w_0, w_1) = 
  w_0 + w_1x_n = 
  \begin{bmatrix}
  w_0 & w_1
  \end{bmatrix}
  \begin{bmatrix}
  1 \\ x_1
  \end{bmatrix} = 
  \mathbf{w}^T\mathbf{x_n}
\] The matrix representing all the input data \(\mathbf{X}\) and the
column vector of outcomes \(\mathbf{t}\) (\(\mathbf{y}\) in econometrics)
are:

\[
\mathbf{X} = 
\begin{bmatrix}
x_{1}^T \\
x_{2}^T \\
\vdots \\
x_{N}^T
\end{bmatrix}
= \begin{bmatrix}
1 & x_{1} \\
1 & x_{2} \\
\vdots & \vdots\\
1 & x_{N}
\end{bmatrix}
\]

\[
\mathbf{t} = 
\begin{bmatrix}
t_{1} \\
t_{2} \\
\vdots \\
t_{N}
\end{bmatrix}
\]

The loss function \(\mathcal{L}\) becomes \[
\mathcal{L} = 
\frac{1}{N}(\mathbf{t} -\mathbf{X})^T(\mathbf{t} -\mathbf{X})
\]

The column vector of values predicted by the function \(f(x_n; w_0, w_1)\)
are:

\[\mathbf{Xw} = 
\begin{bmatrix}
1 & x_{1} \\
1 & x_{2} \\
\vdots & \vdots \\
1 & x_{N}
\end{bmatrix}
\times
\begin{bmatrix}
  w_0 \\ w_1
\end{bmatrix} =
\begin{bmatrix}
  w_0 & w_1x_1 \\
  w_0 & w_1x_2 \\
  \vdots & \vdots \\
  w_0 & w_1x_N 
 \end{bmatrix}
\]

\[
\underset{(N \times 2)}{\mathbf{X}} \times \underset{(2 \times 1)}{\mathbf{w}} =
    \underset{(N \times 1)}{\mathbf{Xw}}
\] The column vector of residual is

\[
\mathbf{t} - \mathbf{Xw} = 
\begin{bmatrix}
  t_1 - w_0 + w_1x_1 \\
  t_2 - w_0 + w_1x_2 \\
  \vdots \\
  t_N - w_0 + w_1x_N 
 \end{bmatrix}
\]

The sum of squared errors can be expressed as:
\[\sum_{n=1}^{N} (t_n - (w_0 + w_1x_1))^2 \\\]

\[ =
\begin{bmatrix}
  t_1 - (w_0 + w_1x_1) &
  t_2 - (w_0 + w_1x_2) &
  ... &
  t_N - (w_0 + w_1x_N )
\end{bmatrix} 
 \times
\begin{bmatrix}
  t_1 - (w_0 + w_1x_1) & \\
  t_2 - (w_0 + w_1x_2) & \\
  \vdots \\
  t_N - (w_0 + w_1x_N )
\end{bmatrix} \\
= (\mathbf{t} - \mathbf{Xw})^T(\mathbf{t} - \mathbf{Xw})\]

We can now write our loss function compactly with matrix notation, which
produces a scalar \((1 \times 1)\) value:

\[
\underset{(1 \times 1)}{\frac{1}{N}}
\underset{(1 \times N)}{(\mathbf{t} - \mathbf{Xw})^T}
\underset{(N \times 1)}{(\mathbf{t} - \mathbf{Xw})}
= 
\underset{(1 \times 1)}{\mathcal{L}}
\] We can expand the sum of squared errors as follows:

\[
(\mathbf{t} - \mathbf{Xw})^T(\mathbf{t} - \mathbf{Xw})\]

\[= (\mathbf{t}^T - (\mathbf{Xw})^T)(\mathbf{t} - \mathbf{Xw})\]

\[=(\mathbf{t}^T - (\mathbf{w}^T \mathbf{X}^T))(\mathbf{t} - \mathbf{Xw})\]

\begin{equation}
=\underset{(1 \times 1)}{\mathbf{t}^T\mathbf{t}} -
\underset{(1 \times N)}{\mathbf{t}^T}
\underset{(N \times 2)}{\mathbf{X}}
\underset{(2 \times 1)}{\mathbf{w}} -
\underset{(1 \times 2)}{\mathbf{w}^T}
\underset{(2 \times N)}{\mathbf{X}^T}
\underset{(N \times 1)}{\mathbf{t}} +
\underset{(1 \times 2)}{\mathbf{w}^T}
\underset{(2 \times N)}{\mathbf{X}^T}
\underset{(N \times 2)}{\mathbf{X}}
\underset{(2 \times 1)}{\mathbf{w}}
\end{equation}

Note the following:

\begin{itemize}
\item
  each of the term becomes \(1 \times 1\)
\item
  the terms \(\mathbf{t}^T \mathbf{Xw}\) and
  \(\mathbf{w}^T \mathbf{X}^T\mathbf{t}\) are transposes of each other
  and are scalars. So they are the same value and can be combined.
\end{itemize}

So, sum of squared residuals can be written as
\[\mathbf{t}^T\mathbf{t} - 2\mathbf{w}^T\mathbf{X}^T\mathbf{t} +
\mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w}\]

Hence, the loss function becomes:

\[\mathcal{L} = \frac{1}{N}\mathbf{t}^T\mathbf{t} - 2\mathbf{w}^T\mathbf{X}^T\mathbf{t} +
\mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w}\]

Now that we have defined our loss function in matrix notation, we still
want to find the value of w that will minimize the loss. Like before,
this involves taking the derivative (now with respect to the vector
\(\mathbf{w}\)), setting the derivative equal to 0, and then solving for
\(\mathbf{w}\). Thus, we must delve into doing vector Calculus.

\hypertarget{review-derivatives}{%
\section{Review: Derivatives}\label{review-derivatives}}

Lets say we have a function of one variable that produces a scalar
output (real-valued input is mapped to real-valued output):

\[
f:
\mathbb{R} 
\rightarrow 
\mathbb{R}
\] The derivative of this function tells us how the function changes
with respect to that variable. The derivative is:

\[
f^{\prime}(x) = 
\lim_{h\to 0} 
\frac{f(x + h) - f(x)}{h}
\]

Which can be rearranged to say that the derivative at a \(f^{\prime}(x)\)
satisfies the following:

\[
\lim_{h\to 0} 
\frac{f(x + h) - f(x) - f^{\prime}(x)h}
{h} 
= 0
\]

\textbf{Gradient}

Let's say we have a function of a vector that produces a scalar output
(n-dimensional real-valued input is mapped to one real-valued output):

\[
f:
\mathbb{R}^{n} 
\rightarrow 
\mathbb{R}
\]

The gradient \(\nabla f(x)\) is the multidimensional derivative and it
tells us how the function changes with respect to each component in the
vector. It satisfies the following equation:

\[
\lim_{\| h \|\to 0} 
\frac{\|f(x + h) - f(x) - f^{\prime}(x)h\|}
{\| h \|}
= 0
\]

Again, we have a function of a vector that produces a scalar
\(f:\mathbb{R}^{n} \rightarrow\mathbb{R}\). The input to the function is a
vector:

\[\mathbf{w} = (w_1, w_2, ..., w_n)^T\]

The gradient of the function with respect to \(\mathbf{w}\) at
\(\mathbf{w}\) is itself a vector:

\[
\nabla f(\mathbf{w}) =
\frac{\partial{f}}{\partial{\mathbf{w}}} =
(
\frac{\partial{f}}{\partial{w_1}},
\frac{\partial{f}}{\partial{w_2}}, ...,
\frac{\partial{f}}{\partial{w_n}}
)^T =
\begin{bmatrix}
  \frac{\partial{f}}{\partial{w_1}} \\
  \frac{\partial{f}}{\partial{w_2}} \\
  \vdots \\
  \frac{\partial{f}}{\partial{w_n}}
 \end{bmatrix}
\] Like the derivative, the gradient represents the slope of the tangent
of the graph of the function. More precisely, the gradient points in the
direction of the greatest rate of increase of the function, and its
magnitude is the slope of the graph in that direction.

The gradient will always have the same dimensions as the input vector.
In the following examples, \(\mathbf{w}\) and \(\mathbf{w}\) are 2 x 1
matrices.

\includegraphics[width=18.24in]{images/derivatives}

\hypertarget{back-to-slr}{%
\section{Back to SLR}\label{back-to-slr}}

Earlier, we found the total OLS regression loss function to be:

\[\mathcal{L} = \frac{1}{N}\mathbf{t}^T\mathbf{t} - 2\mathbf{w}^T\mathbf{X}^T\mathbf{t} +
\mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w}\]

We will take the derivative of the loss with respect to the vector \(\mathbf{w}\), set the derivative equal to
the \(\mathbf{0}\) vector, and then solve for \(\mathbf{w}\).

If \(\mathbf{w}\) is a \(2 \times 1\) vector, then \(\nabla{f}\) must also be \(2 \times 1\).

Now note that \(\underset{(1 \times 2)}{\mathbf{w}^T}\underset{(2 \times N)}{\mathbf{X}^T}\underset{(N \times 1)}{\mathbf{t}}\) could be treated as \(\underset{(1 \times 2)}{\mathbf{w}^T}\underset{(2 \times 1)}{\mathbf{x}}\)

\[
\frac{\partial f}{\partial\mathbf{w}} =
0 - 
\frac{N}{2}X^T\mathbf{t} +
\frac{N}{2}X^TX\mathbf{w}
\]

  \bibliography{book.bib,packages.bib}

\end{document}
