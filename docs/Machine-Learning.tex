% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Machine Learning},
  pdfauthor={Bhaswar Chakma},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Machine Learning}
\author{Bhaswar Chakma}
\date{2021-07-06}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{section}{%
\chapter*{}\label{section}}
\addcontentsline{toc}{chapter}{}

\href{https://cloud.google.com/products/ai/ml-comic-1/}{Source: Google ML Comic}

\hypertarget{ols}{%
\chapter{OLS}\label{ols}}

\hypertarget{slr}{%
\section{SLR}\label{slr}}

We define our column vector of parameters \(\mathbf{w}\) (\(\boldsymbol{\beta}\) in econometrics):

\[\mathbf{w} = \begin{bmatrix}
           w_{0} \\
           w_{1} 
         \end{bmatrix}\]

A single observation \(\mathbf{x_n}\) is:
\[
  \mathbf{x_n} =
  \begin{bmatrix}
          1 \\
          x_{n} 
  \end{bmatrix}
\]
And thus the value predicted by the model can be written as:

\[
  f(x_n; w_0, w_1) = 
  w_0 + w_1x_n = 
  \begin{bmatrix}
  w_0 & w_1
  \end{bmatrix}
  \begin{bmatrix}
  1 \\ x_1
  \end{bmatrix} = 
  \mathbf{w}^T\mathbf{x_n}
\]
The matrix representing all the input data \(\mathbf{X}\) and the column vector of
outcomes \(\mathbf{t}\) (\(\mathbf{y}\) in econometrics) are:

\[
\mathbf{X} = 
\begin{bmatrix}
x_{1}^T \\
x_{2}^T \\
\vdots \\
x_{N}^T
\end{bmatrix}
= \begin{bmatrix}
1 & x_{1} \\
1 & x_{2} \\
\vdots & \vdots\\
1 & x_{N}
\end{bmatrix}
\]

\[
\mathbf{t} = 
\begin{bmatrix}
t_{1} \\
t_{2} \\
\vdots \\
t_{N}
\end{bmatrix}
\]

The loss function \(\mathcal{L}\) becomes
\[
\mathcal{L} = 
\frac{1}{N}(\mathbf{t} -\mathbf{X})^T(\mathbf{t} -\mathbf{X})
\]

The column vector of values predicted by the function \(f(x_n; w_0, w_1)\) are:

\[\mathbf{Xw} = 
\begin{bmatrix}
1 & x_{1} \\
1 & x_{2} \\
\vdots & \vdots \\
1 & x_{N}
\end{bmatrix}
\times
\begin{bmatrix}
  w_0 \\ w_1
\end{bmatrix} =
\begin{bmatrix}
  w_0 & w_1x_1 \\
  w_0 & w_1x_2 \\
  \vdots & \vdots \\
  w_0 & w_1x_N 
 \end{bmatrix}
\]

\[
\underset{(N \times 2)}{\mathbf{X}} \times \underset{(2 \times 1)}{\mathbf{w}} =
    \underset{(N \times 1)}{\mathbf{Xw}}
\]
The column vector of residual is

\[
\mathbf{t} - \mathbf{Xw} = 
\begin{bmatrix}
  t_1 - w_0 + w_1x_1 \\
  t_2 - w_0 + w_1x_2 \\
  \vdots \\
  t_N - w_0 + w_1x_N 
 \end{bmatrix}
\]

The sum of squared errors can be expressed as:
\[\sum_{n=1}^{N} (t_n - (w_0 + w_1x_1))^2 \\\]

\[ =
\begin{bmatrix}
  t_1 - (w_0 + w_1x_1) &
  t_2 - (w_0 + w_1x_2) &
  ... &
  t_N - (w_0 + w_1x_N )
\end{bmatrix} 
 \times
\begin{bmatrix}
  t_1 - (w_0 + w_1x_1) & \\
  t_2 - (w_0 + w_1x_2) & \\
  \vdots \\
  t_N - (w_0 + w_1x_N )
\end{bmatrix} \\
= (\mathbf{t} - \mathbf{Xw})^T(\mathbf{t} - \mathbf{Xw})\]

We can now write our loss function compactly with matrix notation, which produces a scalar
\((1 \times 1)\) value:

\[
\underset{(1 \times 1)}{\frac{1}{N}}
\underset{(1 \times N)}{(\mathbf{t} - \mathbf{Xw})^T}
\underset{(N \times 1)}{(\mathbf{t} - \mathbf{Xw})}
= 
\underset{(1 \times 1)}{\mathcal{L}}
\]
We can expand the sum of squared errors as follows:

\[
(\mathbf{t} - \mathbf{Xw})^T(\mathbf{t} - \mathbf{Xw})\]

\[= (\mathbf{t}^T - (\mathbf{Xw})^T)(\mathbf{t} - \mathbf{Xw})\]

\[=(\mathbf{t}^T - (\mathbf{w}^T \mathbf{X}^T))(\mathbf{t} - \mathbf{Xw})\]

\begin{equation}
=\underset{(1 \times 1)}{\mathbf{t}^T\mathbf{t}} -
\underset{(1 \times N)}{\mathbf{t}^T}
\underset{(N \times 2)}{\mathbf{X}}
\underset{(2 \times 1)}{\mathbf{w}} -
\underset{(1 \times 2)}{\mathbf{w}^T}
\underset{(2 \times N)}{\mathbf{X}^T}
\underset{(N \times 1)}{\mathbf{t}} +
\underset{(1 \times 2)}{\mathbf{w}^T}
\underset{(2 \times N)}{\mathbf{X}^T}
\underset{(N \times 2)}{\mathbf{X}}
\underset{(2 \times 1)}{\mathbf{w}}
\end{equation}

Note the following:

\begin{itemize}
\item
  each of the term becomes \(1 \times 1\)
\item
  the terms \(\mathbf{t}^T \mathbf{Xw}\) and \(\mathbf{w}^T \mathbf{X}^T\mathbf{t}\) are transposes of each other and are scalars. So they are the same value and can be combined.
\end{itemize}

So, sum of squared residuals can be written as
\[\mathbf{t}^T\mathbf{t} - 2\mathbf{w}^T\mathbf{X}^T\mathbf{t} +
\mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w}\]

Hence, the loss function becomes:

\[\mathcal{L} = \frac{1}{N}\mathbf{t}^T\mathbf{t} - 2\mathbf{w}^T\mathbf{X}^T\mathbf{t} +
\mathbf{w}^T\mathbf{X}^T\mathbf{X}\mathbf{w}\]

  \bibliography{book.bib,packages.bib}

\end{document}
